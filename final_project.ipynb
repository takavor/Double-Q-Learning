{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas for project:\n",
    "\n",
    "-try larger grid sizes\n",
    "\n",
    "-try UP and RIGHT moves more often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from envs.GridWorld3x3Env import GridWorld3x3Env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.95 # discount factor was 0.95 for all experiments in the original paper\n",
    "MAX_STEPS = 10000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner function implementations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the Double Q-learning paper, the Q-learning and Double Q-learning agents are implemented with an $\\epsilon$-greedy exploration with $\\epsilon = 1/\\sqrt{n(s)}$, where $n(s)$ is the number of times state $s$ is visited by the agent. \n",
    "\n",
    "The learning rate $\\alpha$ is determined by $\\alpha = 1/n(s,a)$ (mode 1) or $\\alpha = 1/n(s,a)^{0.8}$ (mode 2), where $n(s,a)$ corresponds to the number of times a given state-action pair is visited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(env, Q, state, n_s):\n",
    "    '''\n",
    "    Implementation of episolon-greedy exploration as detailed above.\n",
    "    '''\n",
    "    \n",
    "    # if n_s is 0, let epsilon = 1 for random exploration\n",
    "    if n_s[state] == 0:\n",
    "        epsilon = 1\n",
    "    else:\n",
    "        epsilon = 1 / np.sqrt(n_s[state])\n",
    "    \n",
    "    # choose action\n",
    "    if np.random.rand() < epsilon:\n",
    "        # random action\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        # greedy action\n",
    "        max = np.where(np.max(Q[state]) == Q[state])[0]\n",
    "        # in case there are ties, pick randomly\n",
    "        action = np.random.choice(max)\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(n, mode):\n",
    "    '''\n",
    "    Function to calculate the learning rate based on the number of times a state-action pair is visited, and the mode.\n",
    "    '''\n",
    "    if mode == 1:\n",
    "        alpha = 1/n\n",
    "    elif mode == 2:\n",
    "        alpha = 1/n^0.8\n",
    "    return alpha"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(env, n_episodes, mode):\n",
    "    '''\n",
    "    Implementation of the Q-learning algorithm.\n",
    "    '''\n",
    "    \n",
    "    # create Q table\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    rewards_per_episode = np.zeros(n_episodes)\n",
    "\n",
    "    rewards_per_timestep_per_episode = []\n",
    "\n",
    "    for episode in range(0, n_episodes):\n",
    "    # for episode in tqdm(range(n_episodes)):\n",
    "\n",
    "        # keep track of number of times a state is visited (used to calculate epislon)\n",
    "        n_s = np.zeros(env.observation_space.n)\n",
    "\n",
    "        # keep track of number of times a state-action pair is visited (used to calculate alpha)\n",
    "        n_s_a = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "        # reset env\n",
    "        state, _ = env.reset()\n",
    "        \n",
    "        # update n_s for initial state\n",
    "        n_s[state] += 1\n",
    "\n",
    "        # array to keep track of rewards per timestep for this episode\n",
    "        rewards_per_timestep = []\n",
    "\n",
    "        terminated = False\n",
    "\n",
    "        counter = 0\n",
    "\n",
    "        while not terminated:\n",
    "\n",
    "            counter += 1\n",
    "            \n",
    "            # choose action\n",
    "            action = e_greedy(env, Q, state, n_s)\n",
    "\n",
    "            # step and get reward\n",
    "            next_state, reward, terminated, _ = env.step(action)\n",
    "\n",
    "            # update n_s for new state\n",
    "            n_s[next_state] += 1\n",
    "\n",
    "            # update n_s_a for (state, action)\n",
    "            n_s_a[state, action] += 1\n",
    "\n",
    "            # get learning rate alpha\n",
    "            alpha = get_lr(n_s_a[state, action], mode)\n",
    "\n",
    "            # update Q table\n",
    "            Q[state, action] += alpha * (reward + GAMMA * np.max(Q[next_state]) - Q[state, action])\n",
    "\n",
    "            # update state to be next state\n",
    "            state = next_state\n",
    "\n",
    "            rewards_per_timestep.append(reward)\n",
    "            rewards_per_episode[episode] += reward\n",
    "\n",
    "        # episode terminated, append all rewards per timestep to the list\n",
    "        rewards_per_timestep_per_episode.append(rewards_per_timestep)\n",
    "\n",
    "    return rewards_per_timestep_per_episode, rewards_per_episode, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:21<00:00, 123.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# test q-learning\n",
    "\n",
    "rewards_per_experiment = []\n",
    "\n",
    "# do 10000 experiments\n",
    "for i in tqdm(range(10000)):\n",
    "    \n",
    "    env3x3 = GridWorld3x3Env()\n",
    "\n",
    "    rewards_per_timestep, rewards_per_episode, Q = Q_learning(env=env3x3, n_episodes=1, mode=1)\n",
    "\n",
    "    rewards_per_experiment.append(rewards_per_timestep)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Double_Q_learning():\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
